{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9e0a6f",
   "metadata": {},
   "source": [
    "## 라이브러리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e654376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install eli5==0.13.0\n",
    "%pip install eli5==0.13.0\n",
    "%pip install haversine\n",
    "%pip install optuna xgboost\n",
    "\n",
    "\n",
    "# 한글 폰트 사용을 위한 라이브러리입니다.\n",
    "# !apt-get install -y fonts-nanum\n",
    "!apt-get install -y fonts-nanum\n",
    "\n",
    "!pip install lightgbm==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "import seaborn as sns\n",
    "\n",
    "# utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings;warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GroupKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# 추가\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 데이터셋 로드 및 정제\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "# import eli5\n",
    "# from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b336f",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fa35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = '/data/ephemeral/home/house-price/data/merge_subway_fixed.csv'\n",
    "df = pd.read_csv(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4933dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total data shape : ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f26dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51591eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['is_test'] == 0]\n",
    "test_df  = df[df['is_test'] == 1]\n",
    "\n",
    "# 크기 확인\n",
    "print(\"Train 데이터 행 개수:\", train_df.shape[0])\n",
    "print(\"Test 데이터 행 개수:\", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b365b51",
   "metadata": {},
   "source": [
    "## 데이터 전처리 \n",
    "\n",
    "#### 좌표 결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf908cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Train/Test 분리 (이 부분은 동일합니다)\n",
    "# ---------------------------\n",
    "train_df = df[df[\"is_test\"] == 0].copy()\n",
    "test_df  = df[df[\"is_test\"] == 1].copy()\n",
    "\n",
    "print(\"Before:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 훈련 세트(train_df)에서 결측치 처리 규칙 찾기\n",
    "# ---------------------------\n",
    "\n",
    "# (1) 결측치가 많은 행 삭제 (훈련 세트에만 적용)\n",
    "train_df.dropna(subset=[\"좌표X\", \"좌표Y\", \"가까운역이름\", \"거리\"], how=\"all\", inplace=True)\n",
    "\n",
    "# (2) 결측치 대체를 위한 통계값 계산 (✨훈련 데이터'로만' 계산)\n",
    "sigungu_coord_means = train_df.groupby(\"시군구\")[[\"좌표X\", \"좌표Y\"]].transform('mean')\n",
    "sigungu_dist_medians = train_df.groupby(\"시군구\")[\"거리\"].transform('median')\n",
    "global_dist_median = train_df[\"거리\"].median() # 전체 거리 중앙값\n",
    "\n",
    "# (3) 계산된 통계값으로 훈련 데이터(train_df)의 결측치 채우기\n",
    "train_df[\"좌표X\"].fillna(sigungu_coord_means[\"좌표X\"], inplace=True)\n",
    "train_df[\"좌표Y\"].fillna(sigungu_coord_means[\"좌표Y\"], inplace=True)\n",
    "train_df[\"가까운역이름\"].fillna(\"Unknown\", inplace=True)\n",
    "train_df[\"거리\"].fillna(sigungu_dist_medians, inplace=True)\n",
    "train_df[\"거리\"].fillna(global_dist_median, inplace=True) # 시군구 중앙값도 없는 경우 대비\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 테스트 세트(test_df)에 '훈련 세트의 규칙' 적용하기\n",
    "# ---------------------------\n",
    "\n",
    "# (1) 테스트 데이터의 통계값 계산 (✨'훈련 데이터'의 규칙을 적용하기 위해)\n",
    "test_sigungu_coord_means = train_df.groupby('시군구')[['좌표X', '좌표Y']].mean() # 시군구별 평균 좌표\n",
    "test_sigungu_dist_medians = train_df.groupby('시군구')['거리'].median() # 시군구별 거리 중앙값\n",
    "\n",
    "# (2) 테스트 데이터의 시군구별 결측치를 훈련 데이터의 평균/중앙값으로 채우기\n",
    "test_df['좌표X'] = test_df.set_index('시군구')['좌표X'].fillna(test_sigungu_coord_means['좌표X']).values\n",
    "test_df['좌표Y'] = test_df.set_index('시군구')['좌표Y'].fillna(test_sigungu_coord_means['좌표Y']).values\n",
    "test_df['거리'] = test_df.set_index('시군구')['거리'].fillna(test_sigungu_dist_medians).values\n",
    "\n",
    "# (3) 그래도 남은 결측치는 훈련 데이터의 전체 중앙값(global_dist_median)으로 채우기\n",
    "test_df[\"좌표X\"].fillna(train_df['좌표X'].mean(), inplace=True) # 훈련 데이터에 없는 시군구 대비\n",
    "test_df[\"좌표Y\"].fillna(train_df['좌표Y'].mean(), inplace=True) # 훈련 데이터에 없는 시군구 대비\n",
    "test_df[\"가까운역이름\"].fillna(\"Unknown\", inplace=True)\n",
    "test_df[\"거리\"].fillna(global_dist_median, inplace=True)\n",
    "\n",
    "print(\"After:\", train_df.shape, test_df.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 다시 합치기\n",
    "# ---------------------------\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "print(\"최종 데이터셋 크기:\", df.shape)\n",
    "print(\"남은 결측치 수:\\n\", df[[\"좌표X\", \"좌표Y\", \"가까운역이름\", \"거리\"]].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be49fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 2))\n",
    "missing = df.isnull().sum() / df.shape[0]\n",
    "missing = missing[missing > 0]\n",
    "missing.sort_values(inplace=True)\n",
    "missing.plot.bar(color='orange')\n",
    "plt.title('변수별 결측치 비율')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db453f1f",
   "metadata": {},
   "source": [
    "## 이상치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 수치형 컬럼만 선택\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# 긴 형태로 변환 (컬럼명 -> variable, 값 -> value)\n",
    "df_melt = df[num_cols].melt(var_name=\"Feature\", value_name=\"Value\")\n",
    "\n",
    "# 박스플롯\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=df_melt, x=\"Feature\", y=\"Value\")\n",
    "plt.xticks(rotation=90)  # 가독성을 위해 x축 라벨 회전\n",
    "plt.title(\"Boxplots of All Numeric Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae986238",
   "metadata": {},
   "source": [
    "## 파생변수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfe473",
   "metadata": {},
   "source": [
    "#### 시군구 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cdcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_test'].value_counts()     # 또한, train data만 제거되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d71935",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['시군구'].map(lambda x : x.split()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['계약년월'].astype('str').map(lambda x : x[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1621186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시군구, 년월 등 분할할 수 있는 변수들은 세부사항 고려를 용이하게 하기 위해 모두 분할해 주겠습니다.\n",
    "df['구'] = df['시군구'].map(lambda x : x.split()[1])\n",
    "df['동'] = df['시군구'].map(lambda x : x.split()[2])\n",
    "del df['시군구']\n",
    "\n",
    "df['계약년'] = df['계약년월'].astype('str').map(lambda x : x[:4])\n",
    "df['계약월'] = df['계약년월'].astype('str').map(lambda x : x[4:])\n",
    "del df['계약년월']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연속형 변수: ['전용면적', '계약일', '층', '건축년도', 'k-전체동수', 'k-전체세대수', 'k-연면적', 'k-주거전용면적', 'k-관리비부과면적', 'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하', '건축면적', '주차대수', '좌표X', '좌표Y', 'target', '강남여부', '신축여부']\n",
    "# 범주형 변수: ['번지', '본번', '부번', '아파트명', '도로명', 'k-단지분류(아파트,주상복합등등)', 'k-전화번호', 'k-팩스번호', 'k-세대타입(분양형태)', 'k-관리방식', 'k-복도유형', 'k-난방방식', 'k-건설사(시공사)', 'k-시행사', 'k-사용검사일-사용승인일', 'k-수정일자', '고용보험관리번호', '경비비관리형태', '세대전기계약방법', '청소비관리형태', '기타/의무/임대/임의=1/2/3/4', '단지승인일', '사용허가여부', '관리비 업로드', '단지신청일', '구', '동', '계약년', '계약월']\n",
    "del_features = ['부번', '계약일', '계약월']\n",
    "df_ft = df.drop(del_features, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471075d",
   "metadata": {},
   "source": [
    "#### 전용면적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 전용면적 관련 파생변수\n",
    "# ---------------------------\n",
    "\n",
    "# 1) 로그 변환 (큰 값의 분산 완화)\n",
    "df_ft['log_전용면적'] = np.log1p(df_ft['전용면적'])\n",
    "\n",
    "# 2) 전용면적 구간화 (소형/중형/대형/초대형)\n",
    "bins = [0, 60, 85, 135, np.inf]  # m² 기준\n",
    "labels = ['소형', '중형', '대형', '초대형']\n",
    "df_ft['전용면적_구간'] = pd.cut(df_ft['전용면적'], bins=bins, labels=labels)\n",
    "\n",
    "# # 3) 평당가격 (㎡를 평으로 변환 후 계산)\n",
    "# df_ft['평당가격'] = df_ft['target'] / (df_ft['전용면적'] / 3.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b902304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft['단지명'] = df_ft['동'] + \"_\" + df_ft['아파트명']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce54e8",
   "metadata": {},
   "source": [
    "#### 거리 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 두 좌표 사이 거리 계산 (m 단위)\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371000  # 지구 반경 (m)\n",
    "    return c * r\n",
    "\n",
    "closest_names = []\n",
    "closest_distances = []\n",
    "station_counts = []\n",
    "\n",
    "# tqdm으로 진행률 표시\n",
    "for idx, row in tqdm(df_ft.iterrows(), total=len(df_ft), desc=\"Processing\"):\n",
    "    if pd.isna(row['좌표X']) or pd.isna(row['좌표Y']):\n",
    "        closest_names.append(None)\n",
    "        closest_distances.append(None)\n",
    "        station_counts.append(0)\n",
    "        continue\n",
    "    \n",
    "    apt_lon = float(row['좌표X'])\n",
    "    apt_lat = float(row['좌표Y'])\n",
    "    \n",
    "    # 지하철역들과 거리 계산\n",
    "    distances = df.apply(\n",
    "        lambda r: haversine(apt_lon, apt_lat, float(r['경도']), float(r['위도'])), axis=1\n",
    "    )\n",
    "    \n",
    "    # 가장 가까운 역\n",
    "    min_idx = distances.idxmin()\n",
    "    min_dist = distances[min_idx]\n",
    "    min_station = df.loc[min_idx, '역사명']\n",
    "    \n",
    "    # 1km 이내 역 개수\n",
    "    count_within_1km = (distances <= 1000).sum()\n",
    "    \n",
    "    closest_names.append(min_station)\n",
    "    closest_distances.append(int(round(min_dist / 10) * 10))  # 10m 단위 반올림\n",
    "    station_counts.append(count_within_1km)\n",
    "\n",
    "# 새로운 열 추가\n",
    "concat_merge = df_ft.copy()\n",
    "concat_merge['가까운역이름'] = closest_names\n",
    "concat_merge['거리'] = closest_distances\n",
    "concat_merge['1km내역개수'] = station_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08afab12",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301d52e",
   "metadata": {},
   "source": [
    "## 범주형 인코딩 및 lightGBM 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25335f",
   "metadata": {},
   "source": [
    "#### 범주형 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Train/Test 분리\n",
    "# ---------------------------\n",
    "train_df = df_ft[df_ft['is_test'] == 0].drop(columns=['is_test'])\n",
    "test_df  = df_ft[df_ft['is_test'] == 1].drop(columns=['is_test'])\n",
    "\n",
    "X_train = train_df.drop(columns=['target'])\n",
    "y_train = np.log1p(train_df['target'])\n",
    "\n",
    "# 테스트 데이터에서도 동일하게 제거\n",
    "X_test  = test_df.drop(columns=['target'])\n",
    "# X_train 안에서 object 타입 컬럼만 확인\n",
    "object_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "print(\"Object 타입 컬럼:\", object_cols)\n",
    "\n",
    "# 개수 확인\n",
    "print(\"총 개수:\", len(object_cols))\n",
    "\n",
    "# ---------------------------\n",
    "# 2. object → category 변환\n",
    "# ---------------------------\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == \"object\":\n",
    "        X_train[col] = X_train[col].astype(\"category\")\n",
    "        X_test[col]  = X_test[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb0cda",
   "metadata": {},
   "source": [
    "#### 최적화 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d22fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     param = {\n",
    "#         \"objective\": \"regression\",\n",
    "#         \"metric\": \"rmse\",\n",
    "#         \"n_estimators\": 2000,\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "#         \"random_state\": 42\n",
    "#     }\n",
    "#     gbm = lgb.LGBMRegressor(**param)\n",
    "#     gbm.fit(X_train, y_train)\n",
    "#     y_pred = gbm.predict(X_train)\n",
    "#     return mean_squared_error(y_train, y_pred, squared=False)\n",
    "\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(objective, n_trials=30)\n",
    "# print(\"Best params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d9a3c",
   "metadata": {},
   "source": [
    "#### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe473342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# 3. TimeSeriesSplit 학습 + fold별 RMSE 기록\n",
    "# ---------------------------\n",
    "kf = TimeSeriesSplit(n_splits=5)\n",
    "fold_save_files = []\n",
    "cv_results = []  # fold별 성능 저장용\n",
    "\n",
    "for fold_idx, (train_idx, valid_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\n======== {fold_idx}번째 fold 학습 시작 ========\")\n",
    "\n",
    "    X_train_fold, Y_train_fold = X_train.iloc[train_idx].copy(), y_train.iloc[train_idx].copy()\n",
    "    X_valid_fold, Y_valid_fold = X_train.iloc[valid_idx].copy(), y_train.iloc[valid_idx].copy()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Fold 내 luxury_apt 생성 (누수 방지)\n",
    "    # ---------------------------\n",
    "    temp_train = X_train_fold.copy()\n",
    "    temp_train['target'] = np.expm1(Y_train_fold)\n",
    "\n",
    "    if '2023' in temp_train['계약년'].unique():\n",
    "        luxury_condo = temp_train[temp_train['계약년'] == '2023'].groupby('단지명')['target'].mean()\n",
    "    else:\n",
    "        luxury_condo = temp_train.groupby('단지명')['target'].mean()\n",
    "\n",
    "    luxury_list = luxury_condo[luxury_condo >= 200000].index\n",
    "    X_train_fold['luxury_apt'] = X_train_fold['단지명'].isin(luxury_list).astype(int)\n",
    "    X_valid_fold['luxury_apt'] = X_valid_fold['단지명'].isin(luxury_list).astype(int)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 모델 정의 및 학습\n",
    "    # ---------------------------\n",
    "    gbm = lgb.LGBMRegressor(\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=20,\n",
    "        max_depth=6,\n",
    "        min_child_samples=30,       # 추가: 과적합 제어\n",
    "        lambda_l1=1.0,              # 추가: L1 규제\n",
    "        lambda_l2=1.0,              # 추가: L2 규제\n",
    "        subsample=0.7495,\n",
    "        colsample_bytree=0.7565,\n",
    "        n_estimators=10000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    gbm.fit(\n",
    "        X_train_fold, Y_train_fold,\n",
    "        eval_set=[(X_train_fold, Y_train_fold), (X_valid_fold, Y_valid_fold)],\n",
    "        eval_metric=\"rmse\",\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 예측 및 RMSE 계산\n",
    "    # ---------------------------\n",
    "    y_train_pred_log = gbm.predict(X_train_fold, num_iteration=gbm.best_iteration_)\n",
    "    y_valid_pred_log = gbm.predict(X_valid_fold, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "    # exp로 되돌림\n",
    "    y_train_pred_real = np.expm1(y_train_pred_log)\n",
    "    y_valid_pred_real = np.expm1(y_valid_pred_log)\n",
    "    y_train_true_real = np.expm1(Y_train_fold)\n",
    "    y_valid_true_real = np.expm1(Y_valid_fold)\n",
    "\n",
    "    # RMSE 계산\n",
    "    rmse_train_log = np.sqrt(mean_squared_error(Y_train_fold, y_train_pred_log))\n",
    "    rmse_train_real = np.sqrt(mean_squared_error(y_train_true_real, y_train_pred_real))\n",
    "    rmse_valid_log = np.sqrt(mean_squared_error(Y_valid_fold, y_valid_pred_log))\n",
    "    rmse_valid_real = np.sqrt(mean_squared_error(y_valid_true_real, y_valid_pred_real))\n",
    "\n",
    "    cv_results.append({\n",
    "        \"fold\": fold_idx,\n",
    "        \"train_log_RMSE\": rmse_train_log,\n",
    "        \"train_real_RMSE\": rmse_train_real,\n",
    "        \"valid_log_RMSE\": rmse_valid_log,\n",
    "        \"valid_real_RMSE\": rmse_valid_real\n",
    "    })\n",
    "\n",
    "    print(f\"✅ Fold {fold_idx}\")\n",
    "    print(f\"   Train 로그 RMSE: {rmse_train_log:.4f}, Train 원래 RMSE: {rmse_train_real:.2f}\")\n",
    "    print(f\"   Valid 로그 RMSE: {rmse_valid_log:.4f}, Valid 원래 RMSE: {rmse_valid_real:.2f}\")\n",
    "\n",
    "    # 모델 저장\n",
    "    file_name = f\"timeseries_fold{fold_idx}_gbm.pkl\"\n",
    "    joblib.dump(gbm, file_name)\n",
    "    fold_save_files.append(file_name)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 최종 요약\n",
    "# ---------------------------\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(\"\\n======== 교차검증 결과 요약 ========\")\n",
    "print(cv_df)\n",
    "\n",
    "print(\"\\n평균 Valid 로그 RMSE:\", cv_df[\"valid_log_RMSE\"].mean())\n",
    "print(\"평균 Valid 원래 RMSE:\", cv_df[\"valid_real_RMSE\"].mean())\n",
    "print(\"최적(가장 낮은) Valid 로그 RMSE:\", cv_df[\"valid_log_RMSE\"].min())\n",
    "print(\"최적(가장 낮은) Valid 원래 RMSE:\", cv_df[\"valid_real_RMSE\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Feature Importance 분석\n",
    "# ---------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 수정된 부분 시작 ---\n",
    "\n",
    "# ✅ 1. 'luxury_apt'를 포함한 최종 특성 개수로 importances 배열을 초기화합니다.\n",
    "num_final_features = X_train.shape[1] + 1\n",
    "importances = np.zeros(num_final_features)\n",
    "\n",
    "for f in fold_save_files:\n",
    "    model = joblib.load(f)\n",
    "    # 이제 양쪽의 shape가 (24,) (24,)로 동일해져서 오류가 발생하지 않습니다.\n",
    "    importances += model.feature_importances_\n",
    "\n",
    "importances /= len(fold_save_files)\n",
    "\n",
    "# ✅ 2. DataFrame을 만들 때 사용할 최종 특성 이름 리스트를 만듭니다.\n",
    "# 기존 X_train의 컬럼에 'luxury_apt'를 추가합니다.\n",
    "final_feature_names = X_train.columns.tolist() + ['luxury_apt']\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": final_feature_names, # 수정된 특성 이름 리스트 사용\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# --- 수정된 부분 끝 ---\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=importance_df.head(20), x=\"importance\", y=\"feature\")\n",
    "plt.title(\"Top 20 Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88be606",
   "metadata": {},
   "source": [
    "## 출력파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef633ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = []\n",
    "# for f in fold_save_files:\n",
    "#     model = joblib.load(f)\n",
    "#     preds.append(model.predict(X_test, num_iteration=model.best_iteration_))\n",
    "\n",
    "# y_test_pred = np.expm1(np.mean(preds, axis=0))\n",
    "\n",
    "# submission = pd.read_csv(\"/data/ephemeral/home/house-price/data/sample_submission.csv\")\n",
    "# submission[\"target\"] = np.round(y_test_pred).astype(int)\n",
    "# submission.to_csv(\"submission_lgb_log.csv\", index=False)\n",
    "\n",
    "# print(\"✅ 제출 파일 저장 완료: submission_lgb_log.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
